{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd  \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt  \n",
    "import numpy as np \n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Directory containing CSV files\n",
    "directory = '/home/mohammad/Desktop/projectLLM/dataset520'\n",
    "\n",
    "# List all CSV files in the directory\n",
    "csv_files = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "# Shuffle the list randomly\n",
    "random.shuffle(csv_files)\n",
    "\n",
    "# Initialize an empty DataFrame to store merged data\n",
    "merged_data = pd.DataFrame()\n",
    "\n",
    "# Loop through each CSV file, read it, and merge into the DataFrame\n",
    "for file in csv_files:\n",
    "    data = pd.read_csv(file)\n",
    "    merged_data = pd.concat([merged_data, data], ignore_index=True)\n",
    "\n",
    "# Write the merged data to a new CSV file\n",
    "merged_data.to_csv('/home/mohammad/Desktop/projectLLM/merged_random.csv', index=False)\n",
    "\n",
    "print(\"Merged data saved to merged_random.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/mohammad/Desktop/projectLLM/dataset520/dataset.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data.drop(data.columns[[0, 1, 5]], axis=1, inplace=True)\n",
    "\n",
    "data.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Columns 0, 1, and 5 cleared in the CSV file:\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colsofinterest = [\"src_port\" , \"flow_duration\" , \"flow_byts_s\" , \"flow_pkts_s\" , \"fwd_pkts_s\" , \"tot_fwd_pkts\" , \"totlen_fwd_pkts\" , \"fwd_header_len\" , \"fwd_act_data_pkts\" , \"flow_iat_mean\" , \"flow_iat_max\" ,\"flow_iat_min\" ,\"flow_iat_std\" , \"fwd_iat_tot\" ,\"fwd_iat_max\" ,\"fwd_iat_min\" , \"fwd_iat_mean\" ,\"fwd_iat_std\" ,\"fwd_byts_b_avg\" ,\"fwd_pkts_b_avg\" ,\"fwd_blk_rate_avg\" ,\"subflow_fwd_pkts\" ,\"subflow_fwd_byts\" ] \n",
    "len(colsofinterest) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/mohammad/Desktop/projectLLM/dataset520/dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "all_data = pd.concat([df[col] for col in colsofinterest]) \n",
    "percentiles = np.percentile(all_data, np.arange(1, 101))\n",
    "quartiles = percentiles.tolist()\n",
    "print(f\"number of quartiles: {len(quartiles)}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (0.0, 0.11244552352705767)\n",
    "b = (0.11244552352705767, 0.6673535845977986)\n",
    "c = (0.6673535845977986, 1.0)\n",
    "d = (1.0, 2.0)\n",
    "e = (2.0, 3.0)\n",
    "f = (3.0, 3.559489622105667) \n",
    "g = (3.559489622105667, 6.0)\n",
    "h = (6.0, 9.581268828803156)\n",
    "i = (9.581268828803156, 11.0)\n",
    "j = (11.0, 17.0)\n",
    "k = (17.0, 20.0)\n",
    "l = (20.0, 40.0)\n",
    "m = (40.0, 60.0)\n",
    "n = (60.0, 74.0)\n",
    "o = (74.0, 89.88980835439897)\n",
    "p = (89.88980835439897, 112.0)\n",
    "q = (112.0, 148.0)\n",
    "r = (148.0, 199.0)\n",
    "s = (199.0, 296.0)\n",
    "t = (296.0, 2521.8522647163813)\n",
    "u = (2521.8522647163813, 4096.0)\n",
    "v = (4096.0, 5780.346820809248)\n",
    "w = (5780.346820809248, 8196.72131147541)\n",
    "x = (8196.72131147541, 9803.921568627453)\n",
    "y = (9803.921568627453, 11264.0)\n",
    "z = (11264.0, 13513.513513513511)\n",
    "a1 = (13513.513513513511, 18122.0)\n",
    "b1 = (18122.0, 28571.42857142857)\n",
    "c1 = (28571.42857142857, 37642.0)\n",
    "d1 = (37642.0, 43838.140000000014)\n",
    "e1 = (43838.140000000014, 51634.0)\n",
    "f1 = (51634.0, 57154.50152098043)\n",
    "g1 = (57154.50152098043, 70181.3128153594)\n",
    "h1 = (70181.3128153594, 187613.0)\n",
    "i1 = (187613.0, 285987.0)\n",
    "j1 = (285987.0, 377213.5700000002)\n",
    "k1 = (377213.5700000002, 507936.50793650793)\n",
    "l1 = (507936.50793650793, 661913.6)\n",
    "m1 = (661913.6, 895104.8951048951)\n",
    "n1 = (895104.8951048951, 1034389.0300000045)\n",
    "o1 = (1034389.0300000045, 1219047.619047619)\n",
    "p1 = (1219047.619047619, 1671964.5127806715)\n",
    "q1 = (1671964.5127806715, 2825195.0)\n",
    "r1 = (2825195.0, 3312339.5000000056)\n",
    "s1 = (3312339.5000000056, 5110005.194339051)\n",
    "t1 = (5110005.194339051, 8259176.825999905)\n",
    "u1 = (8259176.825999905, 14358868.141890742)\n",
    "v1 = (14358868.141890742, 26460339.300000004)\n",
    "w1 = (26460339.300000004, 125961768.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = {\n",
    "    (0.0, 0.11244552352705767): 'a',\n",
    "    (0.11244552352705767, 0.6673535845977986): 'b',\n",
    "    (0.6673535845977986, 1.0): 'c',\n",
    "    (1.0, 2.0): 'd',\n",
    "    (2.0, 3.0): 'e',\n",
    "    (3.0, 3.559489622105667): 'f',\n",
    "    (3.559489622105667, 6.0): 'g',\n",
    "    (6.0, 9.581268828803156): 'h',\n",
    "    (9.581268828803156, 11.0): 'i',\n",
    "    (11.0, 17.0): 'j',\n",
    "    (17.0, 20.0): 'k',\n",
    "    (20.0, 40.0): 'l',\n",
    "    (40.0, 60.0): 'm',\n",
    "    (60.0, 74.0): 'n',\n",
    "    (74.0, 89.88980835439897): 'o',\n",
    "    (89.88980835439897, 112.0): 'p',\n",
    "    (112.0, 148.0): 'q',\n",
    "    (148.0, 199.0): 'r',\n",
    "    (199.0, 296.0): 's',\n",
    "    (296.0, 2521.8522647163813): 't',\n",
    "    (2521.8522647163813, 4096.0): 'u',\n",
    "    (4096.0, 5780.346820809248): 'v',\n",
    "    (5780.346820809248, 8196.72131147541): 'w',\n",
    "    (8196.72131147541, 9803.921568627453): 'x',\n",
    "    (9803.921568627453, 11264.0): 'y',\n",
    "    (11264.0, 13513.513513513511): 'z',\n",
    "    (13513.513513513511, 18122.0): 'a1',\n",
    "    (18122.0, 28571.42857142857): 'b1',\n",
    "    (28571.42857142857, 37642.0): 'c1',\n",
    "    (37642.0, 43838.140000000014): 'd1',\n",
    "    (43838.140000000014, 51634.0): 'e1',\n",
    "    (51634.0, 57154.50152098043): 'f1',\n",
    "    (57154.50152098043, 70181.3128153594): 'g1',\n",
    "    (70181.3128153594, 187613.0): 'h1',\n",
    "    (187613.0, 285987.0): 'i1',\n",
    "    (285987.0, 377213.5700000002): 'j1',\n",
    "    (377213.5700000002, 507936.50793650793): 'k1',\n",
    "    (507936.50793650793, 661913.6): 'l1',\n",
    "    (661913.6, 895104.8951048951): 'm1',\n",
    "    (895104.8951048951, 1034389.0300000045): 'n1',\n",
    "    (1034389.0300000045, 1219047.619047619): 'o1',\n",
    "    (1219047.619047619, 1671964.5127806715): 'p1',\n",
    "    (1671964.5127806715, 2825195.0): 'q1',\n",
    "    (2825195.0, 3312339.5000000056): 'r1',\n",
    "    (3312339.5000000056, 5110005.194339051): 's1',\n",
    "    (5110005.194339051, 8259176.825999905): 't1',\n",
    "    (8259176.825999905, 14358868.141890742): 'u1',\n",
    "    (14358868.141890742, 26460339.300000004): 'v1',\n",
    "    (26460339.300000004, 125961768.0): 'w1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/mohammad/Desktop/projectLLM/dataset520/dataset.csv'\n",
    "\n",
    "def map_value_to_letter(value):\n",
    "    for (lower_bound, upper_bound), letter in ranges.items():\n",
    "        if lower_bound <= value < upper_bound:\n",
    "            return letter\n",
    "    return 'Unknown'  # For values that don't fall within any range\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Apply the mapping function to each element in the DataFrame\n",
    "mapped_df = df.applymap(map_value_to_letter)\n",
    "\n",
    "# Concatenate the letters in each row to form words, separated by a space\n",
    "words = mapped_df.apply(lambda row: ' '.join(row), axis=1)\n",
    "\n",
    "# Save the resulting Series to a text file, each \"word\" on a new line\n",
    "with open('mapped_words.txt', 'w') as f:\n",
    "    for word in words:\n",
    "        f.write(f\"{word}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "# Alphabet mapping based on the provided ranges\n",
    "chars = [\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j',\n",
    "    'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't',\n",
    "    'u', 'v', 'w', 'x', 'y', 'z', 'a1', 'b1', 'c1', 'd1',\n",
    "    'e1', 'f1', 'g1', 'h1', 'i1', 'j1', 'k1', 'l1', 'm1',\n",
    "    'n1', 'o1', 'p1', 'q1', 'r1', 's1', 't1', 'u1', 'v1', 'w1'\n",
    "] \n",
    "print(len(chars))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {s:i+1 for i,s in enumerate(chars)}   \n",
    "stoi[\".\"] = 0   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = {i:s for s,i in stoi.items()}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'mapped_words.txt'\n",
    "bigram = {}\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Prepend a dot to the beginning of the line to handle the first word\n",
    "        lst = [\".\"] + line.split()\n",
    "        # Iterate through the words in lst to create bigrams\n",
    "        for i in range(len(lst) - 1):  # Adjusted to use range(len(lst) - 1)\n",
    "            if bigram.get((lst[i], lst[i+1])) is None:\n",
    "                bigram[(lst[i], lst[i+1])] = 1\n",
    "            else:\n",
    "                bigram[(lst[i], lst[i+1])] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "N = torch.zeros(50, 50) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_index = stoi.get('Unknown', 0)  # Get the index for 'Unknown' or use -1 if not found\n",
    "\n",
    "for (char1, char2), freq in bigram.items():\n",
    "    index1 = stoi.get(char1, unknown_index)  # Use get to handle missing keys\n",
    "    index2 = stoi.get(char2, unknown_index)\n",
    "    \n",
    "    # Proceed only if both characters are known\n",
    "    if index1 != -1 and index2 != -1:\n",
    "        N[index1, index2] = freq  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,50))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "for i in range(50):\n",
    "    for j in range(50):\n",
    "        chstr = itos[i] + itos[j]\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
    "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\",\n",
    "color='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+1).float()\n",
    "P /= P.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming P is a list of probability distributions\n",
    "for i, p in enumerate(P):\n",
    "    if not (0.999 <= sum(p) <= 1.001):  # Allow for a small error due to floating point precision\n",
    "        print(f\"P[{i}] is not a valid probability distribution. Sum = {sum(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g1 a a a o m a b b b b n g1 a a a a a a a a d d o m m a a a a b n g1 o m n i p o a a z j a a a a a a a e e d d d o o a a a o m a a a a o a a a a a n1 a1 d d d n h\n",
      "g1 a a m m m a a l1 a y a a a a d a a a a a a a a t a a a a a a a a a a a a a a d d a a a a o a a m m m a a a a t t a a m m a a a a a u a a m a o h p p1 a a a a\n",
      "g1 a o m m m m a a v v d n i p l l l l l l a a a a a a a a a a a o o m n m n g1 a a a a a a a a a a a a b a a a a a a a o m m m m o m m o a a a e e s l1 a a a a\n",
      "d1 o o o o m m a a a o h o a a t a a a a a a a a a a a a a a a a a t j1 w a a a a a a a a a a a a a a a a a a a a a a a a a a a a d d n i a a a a t a a t e d d\n",
      "f1 o o m a a a a a a a a a a a a a a a a a a a a a d a a a a a e r p l a a k1 j1 h1 m1 z w w w w d r m n g1 a t t a a a a o a a o m m l l a a a a a a a a a e r m1 n1\n",
      "d1 o a a a a a a u1 o m m m a c e a a a a a a h q p1 l1 q1 t1 a d d n h j a a a o m n g1 a a d d d d a j a a h o1 p1 r m1 n1 c1 o a a a a a a a a d o h p p1 b1 y a a a t1 v1\n",
      "c1 o a f e r p l a a b e e e e d n g1 a a a a a a a a v d a a a a a a a a a a a a a a a u a f a a a a a a a a a a a a m l a a a o m m a a a a a a a a a a h q n1\n",
      "e1 b1 x a a a a o o o m a a a j a a a a a a a a a a a a a d o m m n g1 a a d o a k s1 t m a a t a a f f e t t t t a a a a a a a a a a a a a b n g1 a a a a a a a t\n",
      "f1 o a a d d d o m a a a a a a a a a a a m a a a a a a a a a h r1 q1 t1 a a e r p m a a a a a a a a d o m o m a a a a o m m a a a a a a a a a a a a a a a a m o h\n",
      "f1 o h o1 l1 a a a u1 q1 u a a a a a a a a a a a a a d c t1 w1 p l l a o o o h s r m m n i p n1 h1 h1 f1 o o m m a a a a a a a a a a a a a a a a a a a a a a a d o a a a\n",
      "g1 a d d o h w1 o o m m m a a a a a a a a a a a a m a a a a a a a a a a a a a o m m o a a a f a m m n g1 a a a a d o o o m m o a h o o o k r1 l1 n1 a1 w w w d d n h\n",
      "g1 a a a a a a a a a a a a a a o m a a a a a d o m o m m n i p u1 w1 w1 h t t t a a a a a a a a a a b1 z a a z j a a a a a a z a a a a a a a a a a a m l a a a a a\n",
      "f1 o o a a a a a a a a a a a a e t t t t a a a a j a a a a a a a o m m a a a a a a a a a a a a t t t t a a a a a a g a a a a a a a a a a a a a a a a a a e r m1\n",
      "g1 a a a a1 a a a a o a a a v1 q o1 q n1 i1 k1 a a a a t t t a t t t a a o m a a a a a a a a a d d o o m a a a a a d c e t a d o m a a a a a a a a a a h o o m m n g1\n",
      "g1 a a a a a a a a a a a a a o m a a a a a a a a d t a a a a a a a a o a m l a a a a a o o m n g1 a a a a a a m l l l l l a a a b n i p l a a f a a a a a o o h\n",
      "f1 o h q o1 a1 d o m o k q1 a1 w d o a a a a a a a a a a a a o m m a a a a a a a a o o m m a a a a a a a a a a o k h1 a a a a t t t t a a h h1 s1 v a m n l l l a a a\n",
      "e1 o o m o h j a a a a a d o h o o m m n i p l l l l l a a a a m m m a a a o m m m n g1 r1 l1 x x x x d o n i p m m a o m m m a a a a a a a a t a a a a a a a a a\n",
      "e1 h1 h1 j1 b1 x d d a a a z a a a a a a a a o m m a d d d a a h j a d o m o m m o o l l a o o m o o h w1 p m l a a a a a a a a a h o m a d o h p l l a i p o a a a\n",
      "c1 o a a a a a a a a a a a a a a a a a o m n i p l l l a w w u f f e d a a m m m a t a a a a o h o m a a a a a a a a a a a a a a a a a a a m o m a a a a a e s\n",
      "e1 o o o o m n g1 a a a m m m a a a a a a a a a a a d d n g1 a a a a a a z d a a a a a a a a a a t t t d a a a a a a a a a a a a a a a a a a a a a a a h q o1 b1 a\n",
      "\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "s = \"\"\n",
    "for _ in range(20):  # Generate 20 sequences\n",
    "    out = []\n",
    "    ix = 0  # Initialize starting index, if applicable\n",
    "    while len(out) < 79:  # Enforce exactly 79 characters\n",
    "        p = P[ix]  # Probability distribution for the current index\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True).item()  # Sample next index\n",
    "        out.append(itos[ix])  # Append character to the sequence\n",
    "\n",
    "        # Optional: Check if 'itos[ix]' is not contributing to the length, handle accordingly\n",
    "        # For example, if 'itos[ix]' could return a non-character or an empty string, adjust the logic here\n",
    "\n",
    "    s += \" \".join(out) + \"\\n\"  # Join characters to form a sequence\n",
    "print(s) \n",
    "examine = s.split(\"\\n\") \n",
    "s = examine[0] \n",
    "checker = s.split(\" \") \n",
    "print(len(checker)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_keys_values(dictionary):\n",
    "    swapped_dict = {value: key for key, value in dictionary.items()}\n",
    "    return swapped_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
